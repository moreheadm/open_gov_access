# Open Government Access

> Aggregate and analyze SF Board of Supervisors votes and meeting data

A hackathon project for Mission Local's challenge to supercharge civic data investigations. This system scrapes, processes, and exposes SF Board of Supervisors voting records through a REST API.

## ğŸ¯ Project Goals

- **Aggregate** Board of Supervisors votes and meeting events
- **Standardize** data from multiple sources via a generic scraping framework
- **Expose** data through a REST API for journalists and civic applications
- **Enable** quick lookups like "find all votes related to a particular property"

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SF BOS Website â”‚  (sfbos.org)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scraper        â”‚  Generic framework with incremental scraping
â”‚  (SF BOS impl)  â”‚  State management for efficiency
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ETL Pipeline   â”‚  PDF â†’ Text â†’ Markdown
â”‚                 â”‚  Extract voting data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL DB  â”‚  Meetings, Documents, Supervisors, Items, Votes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI        â”‚  REST API with auto-generated docs
â”‚  (port 8000)    â”‚  Query supervisors, votes, items, stats
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

**Get up and running in 3 commands:**

```bash
# 1. Install dependencies
cd backend && uv sync

# 2. Start PostgreSQL (requires Docker)
cd .. && docker-compose up -d

# 3. Initialize and run
cd backend && uv run python main.py init && uv run python main.py serve
```

Then visit http://localhost:8000/docs to explore the API!

For detailed instructions, see [RUNNING.md](RUNNING.md).

### Prerequisites

- Python 3.11+
- uv package manager
- PostgreSQL (or Docker)
- [uv](https://github.com/astral-sh/uv) (recommended) or pip

### 1. Install Dependencies

Using `uv` (recommended):
```bash
cd backend
uv sync
```

Or using pip:
```bash
cd backend
pip install -e .
```

### 2. Set Up Database

Create a PostgreSQL database:
```bash
createdb supervisor_votes
```

Set the database URL (optional, defaults to localhost):
```bash
export DATABASE_URL="postgresql://user:password@localhost:5432/supervisor_votes"
```

### 3. Initialize Database

```bash
cd backend
python main.py init
```

This creates tables and seeds the database with current supervisors.

### 4. Run the Pipeline

Scrape and process recent meetings:
```bash
python main.py run --limit 5
```

### 5. Start the API Server

```bash
python main.py serve
```

API available at: http://localhost:8000  
Interactive docs: http://localhost:8000/docs

## ğŸ“š CLI Commands

### Initialize Database
```bash
python backend/main.py init
```

### Scrape Documents
```bash
# Scrape 10 most recent meetings
python backend/main.py scrape --limit 10

# Full scrape (not incremental)
python backend/main.py scrape --full

# Save PDFs to directory
python backend/main.py scrape --save ./pdfs
```

### Process Documents (ETL)
```bash
# Process 5 documents
python backend/main.py process --limit 5

# Force re-process
python backend/main.py process --force
```

### Run Full Pipeline
```bash
# Scrape + process in one command
python backend/main.py run --limit 10
```

### Start API Server
```bash
# Default (port 8000)
python backend/main.py serve

# Custom port
python backend/main.py serve --port 3000

# Auto-reload on code changes
python backend/main.py serve --reload
```

### View Statistics
```bash
python backend/main.py stats
```

### Reset Scraper State
```bash
python backend/main.py reset
```

## ğŸ”Œ API Endpoints

### Supervisors

**Get all supervisors**
```http
GET /api/supervisors
```

**Get supervisor details**
```http
GET /api/supervisors/{id}
```

**Get supervisor's voting history**
```http
GET /api/supervisors/{id}/votes?limit=50&offset=0
```

**Get supervisor's statistics**
```http
GET /api/supervisors/{id}/stats
```

Response:
```json
{
  "supervisor": {
    "id": 1,
    "name": "Connie Chan",
    "district": 1,
    "is_active": true
  },
  "total_votes": 150,
  "aye_count": 120,
  "no_count": 25,
  "abstain_count": 3,
  "absent_count": 2,
  "aye_percentage": 80.0
}
```

### Items (Legislation)

**Get voting items**
```http
GET /api/items?limit=50&search=housing
```

**Get item details with all votes**
```http
GET /api/items/{id}
```

### Meetings

**Get meetings**
```http
GET /api/meetings?limit=20
```

### Statistics

**Get overview stats**
```http
GET /api/stats/overview
```

## ğŸ—„ï¸ Database Schema

### Tables

- **meetings** - Board meetings
- **documents** - Meeting documents (agendas, minutes)
- **supervisors** - Board members
- **items** - Legislation items voted on
- **votes** - Individual supervisor votes on items

### Key Features

- **Incremental scraping** - Only processes new documents
- **State management** - Tracks what's been scraped
- **Generic framework** - Easy to add new data sources
- **Type safety** - Pydantic models and SQLAlchemy
- **Auto-generated API docs** - FastAPI Swagger UI

## ğŸ”§ Development

### Project Structure

```
open_gov_access/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py          # FastAPI application
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â””â”€â”€ pipeline.py      # ETL pipeline
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ database.py      # SQLAlchemy models
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ base.py          # Generic scraper framework
â”‚   â”‚   â””â”€â”€ sfbos.py         # SF BOS implementation
â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â””â”€â”€ main.py              # CLI entry point
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ pyproject.toml           # Project dependencies
â””â”€â”€ README.md
```

### Adding a New Data Source

1. Create a new scraper class extending `Scraper`:

```python
from backend.scrapers.base import Scraper, DocumentMetadata, ScrapedDocument

class MySourceScraper(Scraper):
    def source_name(self) -> str:
        return "my_source"
    
    def discover(self, limit=None):
        # Return list of DocumentMetadata
        pass
    
    def fetch(self, doc_meta):
        # Download and return ScrapedDocument
        pass
```

2. Use it:

```python
scraper = MySourceScraper()
documents = scraper.scrape(limit=10)
```

### Environment Variables

Create a `.env` file:

```bash
DATABASE_URL=postgresql://user:password@localhost:5432/supervisor_votes
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO
```

## ğŸ“– Documentation

See the `docs/` directory for detailed documentation:

- `ARCHITECTURE.txt` - System architecture
- `PROJECT_GUIDE.md` - Development guide
- Mission Local challenge documents

## ğŸ¤ Contributing

This is a hackathon project. Contributions welcome!

## ğŸ“„ License

MIT License

## ğŸ™ Credits

Built for Mission Local's Social Impact Hackathon 2025 challenge to improve civic data accessibility.

